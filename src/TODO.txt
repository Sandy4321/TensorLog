Minibatch computation - this turns out to be a mess with theano's limited abilities to handle sparse matrices
 - ComponentwiseVecMulOp and WeightedVec are difficulties
 -- ComponentwiseVecMulOp occurs ONLY when we multiply messages for a variable in BP
 -- WeightedVec occurs ONLY when we multiply all the messages at the 'top level' in BP
 -- if we knew minibatch size K in env could we just start the backward messages as vstacks of K rows?
    then no resizing is needed when you do the multiplication?

 -- or maybe I should just give up and do the scan in the loss function level
 http://deeplearning.net/software/theano/library/scan.html --- except that gives up on
 using bricks, I guess?

examples of finding rows of a sparse matrix in http://www.deeplearning.net/software/theano/library/sparse/:

>>> m.indices[m.indptr[i]:m.indptr[i+1]], m.data[m.indptr[i]:m.indptr[i+1]]
(array([0, 1], dtype=int32), array([7, 8]))
>>> i = 1
>>> m.indices[m.indptr[i]:m.indptr[i+1]], m.data[m.indptr[i]:m.indptr[i+1]]
(array([2], dtype=int32), array([9]))
>>> i = 2
>>> m.indices[m.indptr[i]:m.indptr[i+1]], m.data[m.indptr[i]:m.indptr[i+1]]
(array([], dtype=int32), array([], dtype=int64))


Simple TODO's:
 - add a proof_failed constant so I never get a zero vector?
 - verify tensorlog works on matrix input, not just single rows
 - tensorlog CLI
 - cache matrixdb transposes and preimages
 - program.maxdepth parameter
 - test sparsity of theano stuff --- why is it so slow? is it actually dense?
 - scalability test - compare to real prolog?

Note: fuel seems hard to install....?

Learning/training:
 - textcat toy w/o incoming chains
 -- data/theory prep
 -- matrixdb wrapper around theano shared variables DONE
 --- db contains a parameterDB sub-object, to which it delegates matrix and matrixpreimage requests
 --- matrixpreimage, matrix requests must include a context, eval or expression
     and returns shared.get_value() or just shared
 --- parameterDB also returns a set of parameters
 -- weighted facts/matrices - not needed yet
 -- trylearn.py following logreg.py example....

 - some KBC task
 - some binary predicate tuning task

Bugs:

- p(X,X) isn't handled correctly.  should I split j=0 into (0,'i') and
(0,'o') in the factor graph?  should there actually be 'factors' and
not goals?
 - 

Think thru: 
 - Blocks and Fuel
 - embeddings 
 - unary predicates or binary predicates with a constant (what's the inferred mode for those?)
 - pairs: possibly impossible to do as a gradient.  (note that for classification all I need is
   pair(X,c,Z) which could be done by just constructing the proper matrix in matrixdb),
   or even with pair_with_c(X,Z)
  - oneHot analysis
  - for theano, do envir's need to include some sort of prefix to keep the var namespaces apart?

------------------------------------------------------------------------------
